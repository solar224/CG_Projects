from train_imports import *
SEED = 123

# --- Constants defined at the top of your script ---
ROAD_HALF_W_CONST = 15.0
SAND_HALF_W_CONST = 35.0
MAX_F_ROAD_CONST = 100.0
MAX_F_SAND_CONST = 60.0
MAX_F_GRASS_CONST = 15.0
TRACK_LEN = 1963.0
ROAD_W = ROAD_HALF_W_CONST
SAND_W = SAND_HALF_W_CONST

# --- Reward Function Constants (REVISED AGAIN) ---
# Goal 1: Track Completion
BONUS_LAP          = 30000.0 # å¼·åŠ›å®Œæˆçå‹µ (scaled: +300 -> QNet output clamped to +50)
K_PROGRESS         = 8.0     # æé«˜å‰é€²çå‹µ (scaled: +0.08/meter)

# Goal 2: Speed (Target > 80, Max 125)
TARGET_SPEED_THRESHOLD = 80.0
K_HIGH_SPEED_BONUS = 20.0    # ç¶­æŒé«˜æ–¼80çš„çå‹µ (scaled: up to +0.2)
K_SPEED_MAINTAIN   = 15.0    # ç¶­æŒæ¥è¿‘æœ€é«˜é€Ÿçš„é¡å¤–çå‹µ (scaled: up to +0.15)
PEN_LOW_SPEED      = -5.0    # é€Ÿåº¦ < 64 (80*0.8) çš„æ‡²ç½° (scaled: -0.05) - æ¸›è¼•æ­¤æ‡²ç½°ï¼Œé¼“å‹µç”¨idleæ¸›é€Ÿ
PEN_VERY_LOW_SPEED = -10.0   # é€Ÿåº¦ < 40 (80*0.5) çš„æ‡²ç½° (scaled: -0.1) - åŒä¸Š

# Goal 3: Avoid Sand & Off-Track (CRITICAL for turns)
PEN_SAND_HIT       = -1800.0 # æ¥µé«˜æ‡²ç½° (scaled: -18.0)
PEN_SAND_EACH_FR   = -180.0  # æ¯å¹€åœ¨æ²™åœ°ä¸Šçš„é«˜æ‡²ç½° (scaled: -1.8 per frame)
PEN_OFF_TRACK      = -2500.0 # æœ€é«˜æ‡²ç½° (scaled: -25.0)

# Other Important Penalties
PEN_WRONG_WAY      = -40.0   # å€’è»Šæ‡²ç½° (scaled: -0.4 for 1m backwards)

# --- Improving Turning Behavior ---
# Penalty for large angle with track when on road (discourages being sideways)
MAX_ANGLE_ON_ROAD_FOR_PENALTY = 20.0 # åº¦ (åŸ25-30, æ›´åš´æ ¼)
PEN_LARGE_ANGLE_ON_ROAD = -10.0    # åŠ å¤§æ‡²ç½° (scaled: -0.1)

# Penalty for excessive steering changes (wiggling), especially at speed
PEN_EXCESSIVE_STEERING_CHANGE = -5.0 # åŠ å¤§æ‡²ç½° (scaled: -0.05)
MIN_SPEED_FOR_STEERING_PENALTY = MAX_F_ROAD_CONST * 0.25 # é€Ÿåº¦é–€æª» (e.g., >31)
MAX_HEADING_CHANGE_BEFORE_PENALTY = 8.0 # åº¦/å¹€ (æ›´åš´æ ¼)

# NEW: Penalty for hitting walls/barriers (inferred from sudden speed drop near edges)
PEN_WALL_HIT_IMPACT = -200.0 # (scaled: -2.0)

# --- Fine-tuning / Stability Rewards (Keep very small) ---
K_CENTER           = 0.1     # æ¥µå°çš„å±…ä¸­çå‹µ (scaled: +0.001)
CENTER_POW         = 1.0
K_ALIGN            = 0.05    # æ¥µå°çš„å°é½Šçå‹µ (scaled: +0.0005)
ON_ROAD_BONUS      = 0.01    # æ¥µå¾®å°çš„åœ¨è·¯çå‹µ (scaled: +0.0001)

ALPHA_SAND_ANGLE   = 1.3     # angle_k å½±éŸ¿åŠ›ç•¥å¾®é™ä½


# ACTION_TABLE and ACTION_SPACE should also be defined early if RacingEnvironment uses them in __init__
ACTION_TABLE = [0, 1, 5, 6]  # idle, W, WA, WD
ACTION_SPACE = len(ACTION_TABLE)
STEER_LEFT_ACTIONS  = {2}  # WA çš„ç´¢å¼•æ˜¯ 2 (å°æ‡‰ ACTION_TABLE[2] å³éŠæˆ²å‹•ä½œ 5)
STEER_RIGHT_ACTIONS = {3}  # WD çš„ç´¢å¼•æ˜¯ 3 (å°æ‡‰ ACTION_TABLE[3] å³éŠæˆ²å‹•ä½œ 6)
STRAIGHT_ACTIONS    = {0, 1} # idle å’Œ W çš„ç´¢å¼•


# è¨Šè™Ÿè™•ç†å‡½æ•¸ï¼Œç”¨æ–¼è™•ç†ç¨‹å¼çµ‚æ­¢è¨Šè™Ÿ
def signal_handler(sig, frame):
    print('\næ•ç²åˆ°çµ‚æ­¢è¨Šè™Ÿï¼Œæ­£åœ¨é€€å‡º...')
    # è¨˜éŒ„ä¸­æ–·äº‹ä»¶åˆ°logs.csv
    log_to_csv(
        time=datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        runtime=time.time() - start_time if 'start_time' in globals() else 0,
        reason=f"ç¨‹å¼è¢«çµ‚æ­¢è¨Šè™Ÿä¸­æ–· (è¨Šè™Ÿ: {sig})"
    )
    # æª¢æŸ¥æ˜¯å¦å•Ÿç”¨äº†éƒµä»¶é€šçŸ¥
    if 'email_config' in globals() and email_config['enabled'] and email_config['notify_errors']:
        subject = "è³½è»Šè¨“ç·´é€šçŸ¥: è¨“ç·´è¢«çµ‚æ­¢"
        message = (f"è¨“ç·´ç¨‹å¼è¢«æ‰‹å‹•çµ‚æ­¢æˆ–ç³»çµ±è¨Šè™Ÿä¸­æ–·!\n\n"
                  f"çµ‚æ­¢æ™‚é–“: {time.strftime('%Y-%m-%d %H:%M:%S')}\n"
                  f"çµ‚æ­¢è¨Šè™Ÿ: {sig}\n")
        # å˜—è©¦ç™¼é€é€šçŸ¥
        try:
            send_email_notification(subject=subject, message=message)
        except Exception as e:
            print(f"ç™¼é€çµ‚æ­¢é€šçŸ¥æ™‚å‡ºéŒ¯: {e}")
    print("ç¨‹å¼å·²çµ‚æ­¢")
    sys.exit(0)

# å°‡è¨“ç·´ä¸­æ–·æˆ–å®Œæˆçš„è¨˜éŒ„å­˜åˆ°logs.csv
def log_to_csv(time, runtime, reason):
    """
    åƒæ•¸:
        time (str): ä¸­æ–·æˆ–å®Œæˆçš„æ™‚é–“ (YYYY-MM-DD HH:MM:SS)
        runtime (float): ç¨‹å¼é‹è¡Œç¸½ç§’æ•¸
        reason (str): ä¸­æ–·åŸå› æˆ–å®Œæˆèªªæ˜
    """
    # å°‡ runtime è½‰æˆ HH:MM:SS
    hours, remainder = divmod(runtime, 3600)
    minutes, seconds = divmod(remainder, 60)
    runtime_formatted = f"{int(hours):02d}:{int(minutes):02d}:{int(seconds):02d}"

    logfile = 'logs.csv'
    file_exists = os.path.isfile(logfile) and os.path.getsize(logfile) > 0

    with open(logfile, 'a', newline='', encoding='utf-8-sig') as csvfile:
        fieldnames = ['æ™‚é–“', 'é‹ä½œç¸½æ™‚é–“', 'ä¸­æ–·åŸå› ']
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)

        # ç¬¬ä¸€æ¬¡å¯«å…¥æ™‚å…ˆåŠ æ¨™é¡Œåˆ—
        if not file_exists:
            writer.writeheader()

        # å¯«å…¥æœ¬æ¬¡ç´€éŒ„
        writer.writerow({
            'æ™‚é–“': time,
            'é‹ä½œç¸½æ™‚é–“': runtime_formatted,
            'ä¸­æ–·åŸå› ': reason
        })

    print(f"å·²è¨˜éŒ„äº‹ä»¶åˆ° {logfile}: {reason}")
# è¨­å®šéš¨æ©Ÿç¨®å­ä»¥ç¢ºä¿å¯é‡ç¾æ€§
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed(SEED)

# å•Ÿç”¨ autograd ç•°å¸¸æª¢æ¸¬ï¼Œç”¨æ–¼åµæ¸¬ NaN
torch.autograd.set_detect_anomaly(True)

# æª¢æŸ¥æ˜¯å¦æœ‰GPUå¯ç”¨
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ä½¿ç”¨è¨­å‚™: {device}")

class SingleHeadDQN(nn.Module):
    def __init__(self, state_size, action_size=ACTION_SPACE):
        super().__init__()
        self.feature = nn.Sequential(
            nn.Linear(state_size, 256), nn.SiLU(),
            nn.Linear(256, 256), nn.SiLU(),
            nn.LayerNorm(256)
        )
        # Dueling
        self.value = nn.Sequential(nn.Linear(256, 128), nn.SiLU(), nn.Linear(128, 1))
        self.adv   = nn.Sequential(nn.Linear(256, 128), nn.SiLU(),
                                   nn.Linear(128, action_size, bias=False))
        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None: nn.init.zeros_(m.bias)

    def forward(self, x):
        z = self.feature(x)
        v = self.value(z)
        a = self.adv(z)
        return v + (a - a.mean(dim=1, keepdim=True))   # (B,4)

# æ”¯æŒé›™è¼¸å‡ºAgentçš„ç¶“é©—å›æ”¾è¨˜æ†¶é«”
class ReplayBuffer:
    def __init__(self, capacity): self.buffer = deque(maxlen=capacity)

    
    def add(self, tup): self.buffer.append(tup)   # (s,a,r,s',d)
    
    def sample(self, batch):
        idx = random.sample(range(len(self.buffer)), batch)
        s  = np.array([self.buffer[i][0] for i in idx])
        a  = np.array([[self.buffer[i][1]] for i in idx])
        r  = np.array([[self.buffer[i][2]] for i in idx])
        s2 = np.array([self.buffer[i][3] for i in idx])
        d  = np.array([[self.buffer[i][4]] for i in idx])
        return s,a,r,s2,d

    def __len__(self): return len(self.buffer)
    

# æ–°çš„DualOutputDQNAgentï¼Œä½¿ç”¨åˆ†é›¢çš„è½‰å‘å’Œæ²¹é–€æ§åˆ¶
class DualOutputDQNAgent:
    def __init__(self, state_size, 
                 lr=3e-5, gamma=0.99, epsilon=1.0, epsilon_decay=0.9975, epsilon_min=0.05,
                 buffer_size=200000, batch_size=256, update_target_freq=7500,
                 epsilon_decay_mode='exponential', decay_steps=20000,
                 huber_delta=5.0, grad_clip_norm=5.0, weight_decay=1e-4):
        self.state_size = state_size
        self.gamma = gamma  # æŠ˜æ‰£å› å­
        self.epsilon = epsilon  # æ¢ç´¢ç‡
        self.epsilon_decay = epsilon_decay  # æ¢ç´¢ç‡è¡°æ¸›ï¼ˆæŒ‡æ•¸è¡°æ¸›æ™‚ä½¿ç”¨ï¼‰
        self.epsilon_min = epsilon_min  # æœ€å°æ¢ç´¢ç‡
        self.batch_size = batch_size  # æ‰¹æ¬¡å¤§å°
        self.update_target_freq = update_target_freq  # ç›®æ¨™ç¶²è·¯æ›´æ–°é »ç‡
        self.train_step = 0  # è¨“ç·´æ­¥æ•¸è¨ˆæ•¸å™¨
        self.huber_delta = huber_delta  # Huberæå¤±çš„deltaå€¼ # å°‡ç”± args.huber_delta å‚³å…¥
        self.grad_clip_norm = grad_clip_norm  # æ¢¯åº¦è£å‰ªç¯„æ•¸ # å°‡ç”± args.grad_clip_norm å‚³å…¥
        self.weight_decay = weight_decay  # AdamWæ¬Šé‡è¡°æ¸›åƒæ•¸
        self.action_size = ACTION_SPACE # ä½¿ç”¨å…¨åŸŸ ACTION_SPACE
        # Epsilonè¡°æ¸›æ¨¡å¼è¨­ç½®
        self.epsilon_decay_mode = epsilon_decay_mode  # 'exponential' æˆ– 'linear'
        self.decay_steps = decay_steps  # ç·šæ€§è¡°æ¸›çš„ç¸½æ­¥æ•¸ # å°‡ç”± args.decay_steps å‚³å…¥
        
        # å‰é€²é è¨“ç·´æ¨¡å¼
        self.forward_pretraining_mode = False
        
        # è¨ˆç®—ç·šæ€§è¡°æ¸›çš„åƒæ•¸
        if self.epsilon_decay_mode == 'linear' and self.decay_steps is not None and self.decay_steps > 0:
            self.epsilon_decay_per_step = (self.epsilon - self.epsilon_min) / self.decay_steps
        else:
            self.epsilon_decay_per_step = 0
        # ä¸»è¦ç¶²è·¯å’Œç›®æ¨™ç¶²è·¯
        self.model = SingleHeadDQN(state_size, action_size=ACTION_SPACE).to(device)
        self.target_model = SingleHeadDQN(state_size, action_size=ACTION_SPACE).to(device)
        self.update_target_model()  # åˆå§‹åŒ–ç›®æ¨™ç¶²è·¯æ¬Šé‡
        # ä½¿ç”¨AdamWå„ªåŒ–å™¨ï¼Œæ·»åŠ æ¬Šé‡è¡°æ¸›
        self.optimizer = optim.AdamW(self.model.parameters(), lr=lr, weight_decay=weight_decay) # lr å°‡ç”± args.lr å‚³å…¥
        # ä½¿ç”¨Huberæå¤±(SmoothL1Loss)æ›¿ä»£MSEï¼Œæ›´å¥½åœ°è™•ç†ç•°å¸¸å€¼ï¼Œæ¸›å°‘æ•¸å€¼çˆ†ç‚¸ 
        self.loss_fn = nn.SmoothL1Loss() # ä½¿ç”¨ self.huber_delta
        # ç¶“é©—å›æ”¾è¨˜æ†¶é«”
        self.memory = ReplayBuffer(buffer_size)
        self.rewards_history = []  # è¨˜éŒ„æ¯å€‹episodeçš„ç¸½çå‹µ
        self.episode_count = 0  # è¨˜éŒ„ç•¶å‰è¨“ç·´åˆ°çš„episodeæ•¸
        # è¿½è¸ªæŒ‡æ¨™
        self.q_values_history = []  # è¨˜éŒ„æ¯å€‹episodeçš„å¹³å‡Qå€¼
        self.current_episode_q_values = []  # ç•¶å‰episodeçš„æ‰€æœ‰Qå€¼
        # ç‹€æ…‹æ¨™æº–åŒ–åƒæ•¸
        self.state_means = np.zeros(state_size)
        self.state_stds = np.ones(state_size)
        self.state_normalization_initialized = False
        self.normalization_samples = []
        # è¨˜éŒ„å‰ä¸€å€‹è½‰å‘å‹•ä½œï¼Œç”¨æ–¼è½‰å‘å¹³æ»‘çå‹µè¨ˆç®—
        self.prev_steering_action = None
        # é åˆ†é…ç‹€æ…‹ç·©è¡å€ä»¥é¿å…åè¦†å‰µå»ºæ–°æ•¸çµ„
        self.scaled_state_buffer = np.zeros(state_size, dtype=np.float32)

    def normalize_state(self, state):
        # æ­¤æ–¹æ³•ç¾åœ¨å¯ä»¥ç›´æ¥ä½¿ç”¨åœ¨æª”æ¡ˆé ‚éƒ¨å®šç¾©çš„å…¨åŸŸå¸¸æ•¸
        scaled = np.zeros_like(state, dtype=np.float32)
        for i in range(len(state)):
            if i == 0 or i == 2:  # x, z coordinates
                scaled[i] = state[i] / (500.0 + 1e-6)
            elif i == 1:  # y coordinate (height)
                scaled[i] = state[i] / (100.0 + 1e-6)
            elif i == 3:  # heading (angle)
                scaled[i] = state[i] / (180.0 + 1e-6)
            elif i == 4:  # speed
                scaled[i] = state[i] / (MAX_F_ROAD_CONST + 1e-6) # ä½¿ç”¨å…¨åŸŸ MAX_F_ROAD_CONST
            elif i == 5:  # lateral_offset
                scaled[i] = state[i] / (ROAD_HALF_W_CONST + 1e-6) # ä½¿ç”¨å…¨åŸŸ ROAD_HALF_W_CONST
            elif i == 6:  # nearest_sample_idx
                scaled[i] = state[i] / 1963.0 
            elif i == 7:  # distance_from_center
                scaled[i] = state[i] / (SAND_HALF_W_CONST + 1e-6) # ä½¿ç”¨å…¨åŸŸ SAND_HALF_W_CONST
            elif i == 8:  # angle_with_track
                scaled[i] = state[i] / (90.0 + 1e-6)
        scaled = np.nan_to_num(scaled, nan=0.0)
        return scaled


    def update_target_model(self):
        self.target_model.load_state_dict(self.model.state_dict())

    def act(self, state, training=True):
        # æ¨™æº–åŒ–ç‹€æ…‹
        s = torch.as_tensor(self.normalize_state(state), dtype=torch.float32,
                            device=device).unsqueeze(0)      # === ä¸€æ¬¡ forwardï¼Œå…ˆæ‹¿åˆ° Q ===
        with torch.no_grad():
            q = self.model(s)              # shape (1,6)
            
        # --- Îµ-greedy / å¼·è¿«åŠ é€Ÿ ---
        if training and np.random.rand() < self.epsilon:
            action_id = np.random.randint(self.action_size)
        else:
            action_id = torch.argmax(q, dim=1).item()
            
# å¦‚æœåœ¨å‰é€²é è¨“ç·´æ¨¡å¼ä¸‹ï¼Œå¼·åˆ¶ä½¿ç”¨ç›´ç·šå‹•ä½œ
        if self.forward_pretraining_mode and training: # Ensure 'training' flag is also checked
            p = np.random.rand()
            if p < 0.6:       # 60% W (increased W probability)
                action_id = 1 # ACTION_TABLE[1] is 'W'
            elif p < 0.8:     # 20% WA
                action_id = 2 # ACTION_TABLE[2] is 'WA' (value 5)
            else:             # 20% WD
                action_id = 3 # ACTION_TABLE[3] is 'WD' (value 6)

        # çµ±è¨ˆ
        if torch.isfinite(q).all():
            self.current_episode_q_values.append(q.max().item())

        return ACTION_TABLE[action_id], action_id      # å›å‚³ (åŸå§‹éŠæˆ²å‹•ä½œ, å…§éƒ¨ id)
    
    def remember(self, state, action_id, reward, next_state, done):
        exp = (self.normalize_state(state), action_id, reward,
            self.normalize_state(next_state), done)
        self.memory.add(exp)
    
    def train(self):
        # è‹¥è¨˜æ†¶é«”ä¸­çš„æ¨£æœ¬ä¸è¶³ï¼Œå‰‡ä¸é€²è¡Œå­¸ç¿’
        if len(self.memory) < self.batch_size:
            return 0
        
        # å¾è¨˜æ†¶é«”ä¸­éš¨æ©ŸæŠ½æ¨£ä¸€æ‰¹ç¶“é©—ï¼Œå·²ç¶“è½‰æ›ç‚ºnumpyæ•¸çµ„
        s,a,r,s2,d = self.memory.sample(self.batch_size)
        s   = torch.as_tensor(s,  dtype=torch.float32, device=device)
        a   = torch.as_tensor(a,  dtype=torch.int64,  device=device)
        r_tensor   = torch.as_tensor(r,  dtype=torch.float32, device=device)
        s2  = torch.as_tensor(s2, dtype=torch.float32, device=device)
        d   = torch.as_tensor(d,  dtype=torch.float32, device=device)

        q_pred = self.model(s).clamp_(-50, 50).gather(1, a)      # [-50, 50] å¤ ç”¨äº†
        
        with torch.no_grad():
            # æ³¨æ„ next_q ä¹Ÿè¦ clamp
            next_a  = self.model(s2).argmax(1, keepdim=True)
            next_q  = self.target_model(s2).clamp_(-50, 50).gather(1, next_a)
            tgt_q = r_tensor / 100.0 + (1-d) * self.gamma * next_q


        # ---------- â‘¡ loss ----------
        loss = self.loss_fn(q_pred, tgt_q)

        # ---------- â‘¢ backward â€” æ¢¯åº¦å®‰å…¨é–¥ ----------
        self.optimizer.zero_grad()
        loss.backward()

        # å…ˆåš NaN/Inf æª¢æŸ¥
        for p in self.model.parameters():
            if p.grad is not None and not torch.isfinite(p.grad).all():
                print("âš ï¸ grad contains NaN/Inf â€“ skip update")
                return loss.item()

        torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.grad_clip_norm)
        self.optimizer.step()

        # ç·šæ€§è¡°æ¸›epsilonï¼ˆä»¥æ­¥æ•¸ç‚ºå–®ä½ï¼‰
        if self.epsilon_decay_mode == 'linear' and self.train_step < self.decay_steps and self.epsilon > self.epsilon_min:
            self.epsilon -= self.epsilon_decay_per_step
            self.epsilon = max(self.epsilon, self.epsilon_min)
        
        # æ›´æ–°ç›®æ¨™ç¶²è·¯
        self.train_step += 1
        if self.train_step % self.update_target_freq == 0:
            self.update_target_model()
            print(f"ç›®æ¨™ç¶²è·¯å·²æ›´æ–°ï¼Œè¨“ç·´æ­¥æ•¸: {self.train_step}")
        
        return loss.item()
    
    def decay_epsilon(self):
        """æ ¹æ“šè¨­å®šçš„è¡°æ¸›æ¨¡å¼èª¿æ•´epsilonå€¼"""
        self.episode_count += 1
        
        if self.epsilon_decay_mode == 'exponential':
            # æŒ‡æ•¸è¡°æ¸›: epsilon = epsilon * decay_rateï¼Œä½†ä¸ä½æ–¼æœ€å°å€¼
            if self.epsilon > self.epsilon_min:
                self.epsilon *= self.epsilon_decay
                self.epsilon = max(self.epsilon, self.epsilon_min)  # ç¢ºä¿ä¸æœƒä½æ–¼æœ€å°å€¼
        
        elif self.epsilon_decay_mode == 'linear' and self.decay_steps is not None:
            # åœ¨è¨“ç·´æ™‚æ¯æ­¥è¡°æ¸›ï¼Œè€Œéæ¯å€‹å›åˆ
            pass  # åœ¨trainæ–¹æ³•ä¸­åŸ·è¡Œç·šæ€§è¡°æ¸›
    
    def reset_episode_stats(self):
        """é‡ç½®æ¯å€‹episodeçš„çµ±è¨ˆæ•¸æ“š"""
        self.current_episode_q_values = []
    
    def get_episode_stats(self):
        """
        å›å‚³ç•¶å‰ episode çš„çµ±è¨ˆæ•¸æ“šï¼Œä¿è­‰åŒ…å« avg_q
        """
        # ---- 1. éæ¿¾ NaN ----
        valid_q_vals = [q for q in self.current_episode_q_values if not math.isnan(q)]
        avg_q       = np.mean(valid_q_vals) if valid_q_vals else 0.0
        # ---- 3. å­˜æ­·å² (å¦‚æœéœ€è¦è¦–è¦ºåŒ–) ----
        self.q_values_history.append(avg_q)

        # ---- 4. å›å‚³ ----
        return {'avg_q': avg_q}
    
    def save(self, filepath):
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'target_model_state_dict': self.target_model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'epsilon': self.epsilon,
            'train_step': self.train_step,
            'rewards_history': self.rewards_history
        }, filepath)
        print(f"æ¨¡å‹å·²ä¿å­˜è‡³ {filepath}")
    
    def load(self, filepath):
        if os.path.isfile(filepath):
            checkpoint = torch.load(filepath)
            self.model.load_state_dict(checkpoint['model_state_dict'])
            self.target_model.load_state_dict(checkpoint['target_model_state_dict'])
            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            self.epsilon = checkpoint.get('epsilon', self.epsilon)
            self.train_step = checkpoint.get('train_step', 0)
            self.rewards_history = checkpoint.get('rewards_history', [])
        else:
            print(f"æ‰¾ä¸åˆ°æ¨¡å‹æª”æ¡ˆ: {filepath}")

# éŠæˆ²ç’°å¢ƒæ¥å£

# é›»å­éƒµä»¶é€šçŸ¥åŠŸèƒ½
def send_email_notification(subject, message, recipient_email=None, sender_email=None, password=None, attachments=None):
    """
    ç™¼é€é›»å­éƒµä»¶é€šçŸ¥
    
    åƒæ•¸:
    - subject: éƒµä»¶ä¸»é¡Œ
    - message: éƒµä»¶å…§å®¹
    - recipient_email: æ”¶ä»¶äººéƒµç®±ï¼Œå¦‚æœç‚ºNoneå‰‡ä½¿ç”¨å…¨å±€é…ç½®
    - sender_email: ç™¼ä»¶äººéƒµç®±ï¼Œå¦‚æœç‚ºNoneå‰‡ä½¿ç”¨å…¨å±€é…ç½®
    - password: ç™¼ä»¶äººå¯†ç¢¼ï¼Œå¦‚æœç‚ºNoneå‰‡ä½¿ç”¨å…¨å±€é…ç½®
    - attachments: é™„ä»¶åˆ—è¡¨ï¼Œæ ¼å¼ç‚º [(filename, filepath), ...]
    """
    # ä½¿ç”¨å…¨å±€é…ç½®æˆ–åƒæ•¸å€¼
    if recipient_email is None and 'email_config' in globals() and email_config['recipient']:
        recipient_email = email_config['recipient']
    elif recipient_email is None:
        recipient_email = "C110110157@gmail.com"  # é»˜èªæ”¶ä»¶äºº
    
    if sender_email is None and 'email_config' in globals() and email_config['sender']:
        sender_email = email_config['sender']
    elif sender_email is None:
        sender_email = "training.notification@gmail.com"  # é»˜èªç™¼ä»¶äºº
    
    if password is None and 'email_config' in globals() and email_config['password']:
        password = email_config['password']
    elif password is None:
        # å¦‚æœæ²’æœ‰æä¾›å¯†ç¢¼ï¼Œå‰‡ç„¡æ³•ç™¼é€éƒµä»¶
        print("éŒ¯èª¤: æœªæä¾›éƒµä»¶ç™¼é€å¯†ç¢¼ï¼Œç„¡æ³•ç™¼é€é€šçŸ¥")
        return False
    
    try:
        # å‰µå»ºéƒµä»¶
        msg = MIMEMultipart()
        msg['From'] = sender_email
        msg['To'] = recipient_email
        msg['Subject'] = subject
        
        # æ·»åŠ æ©Ÿå™¨è­˜åˆ¥è¨Šæ¯
        hostname = socket.gethostname()
        ip_address = socket.gethostbyname(hostname)
        system_info = f"Hostname: {hostname}\nIP: {ip_address}\n"
        
        # æ·»åŠ æ­£æ–‡
        body = system_info + "\n" + message
        msg.attach(MIMEText(body, 'plain'))
        
        # æ·»åŠ é™„ä»¶
        if attachments:
            for filename, filepath in attachments:
                if os.path.exists(filepath):
                    with open(filepath, "rb", encoding='utf-8-sig') as f:
                        part = MIMEApplication(f.read(), Name=filename)
                    part['Content-Disposition'] = f'attachment; filename="{filename}"'
                    msg.attach(part)
        
        # é€£æ¥åˆ°SMTPæœå‹™å™¨ä¸¦ç™¼é€éƒµä»¶
        server = smtplib.SMTP('smtp.gmail.com', 587)
        server.starttls()  # å•Ÿç”¨TLSåŠ å¯†
        
        try:
            server.login(sender_email, password)
        except smtplib.SMTPAuthenticationError as auth_error:
            # Gmailèªè­‰éŒ¯èª¤
            print("\n===== Gmailèªè­‰éŒ¯èª¤ =====")
            print("ç”±æ–¼Googleçš„å®‰å…¨æ”¿ç­–ï¼Œæ‚¨éœ€è¦ä½¿ç”¨ã€Œæ‡‰ç”¨ç¨‹å¼å¯†ç¢¼ã€è€Œä¸æ˜¯æ™®é€šçš„Gmailå¯†ç¢¼ã€‚")
            print("è«‹æŒ‰ç…§ä»¥ä¸‹æ­¥é©Ÿè¨­ç½®æ‡‰ç”¨ç¨‹å¼å¯†ç¢¼ï¼š")
            print("1. å‰å¾€ https://myaccount.google.com/security")
            print("2. ç¢ºä¿æ‚¨å·²é–‹å•Ÿã€Œå…©æ­¥é©Ÿé©—è­‰ã€")
            print("3. é»æ“Šã€Œæ‡‰ç”¨ç¨‹å¼å¯†ç¢¼ã€ï¼Œé¸æ“‡ã€Œå…¶ä»–ã€ä¸¦è¼¸å…¥ä¸€å€‹åç¨±(ä¾‹å¦‚ã€Œè¨“ç·´ç¨‹å¼ã€)")
            print("4. è¤‡è£½ç”Ÿæˆçš„16ä½å­—ç¬¦å¯†ç¢¼")
            print("5. ä½¿ç”¨æ­¤å¯†ç¢¼ä½œç‚º--email_passwordåƒæ•¸")
            print("\néŒ¯èª¤è©³æƒ…:", str(auth_error))
            print("============")
            server.quit()
            return False
            
        text = msg.as_string()
        server.sendmail(sender_email, recipient_email, text)
        server.quit()
        
        print(f"é›»å­éƒµä»¶é€šçŸ¥å·²ç™¼é€è‡³ {recipient_email}")
        return True
    except smtplib.SMTPException as e:
        print(f"ç™¼é€é›»å­éƒµä»¶æ™‚å‡ºéŒ¯ (SMTPéŒ¯èª¤): {str(e)}")
        return False
    except socket.gaierror:
        print(f"ç™¼é€é›»å­éƒµä»¶æ™‚å‡ºéŒ¯: ç„¡æ³•é€£æ¥åˆ°SMTPæœå‹™å™¨ï¼Œè«‹æª¢æŸ¥ç¶²çµ¡é€£æ¥")
        return False
    except Exception as e:
        print(f"ç™¼é€é›»å­éƒµä»¶æ™‚å‡ºéŒ¯: {str(e)}")
        return False

class RacingEnvironment:
    def __init__(self, exe_path="racing.exe", debug=False):
        self.total_samples = None     #  â†  å…ˆç•™ç©º
        self.exe_path = exe_path
        self.state_size = 9  # å¯¦éš›éŠæˆ²è¿”å›çš„ç‹€æ…‹ç¶­åº¦: [x, y, z, heading, speed, lateral_offset, nearest_sample_idx, distance_from_center, angle_with_track]

        self.action_size = ACTION_SPACE # Uses global ACTION_SPACE
        # self.model = SingleHeadDQN(self.state_size, self.action_size).to(device) # Consider moving model creation outside env

        # self.model = SingleHeadDQN(self.state_size, self.action_size).to(device)
        self.process = None
        self.lap_time = 0
        self.travel_distance = 0
        self.last_checkpoint = 0
        self.lap_count = 0
        self.game_interface = GameInterface() # Assuming GameInterface is defined/imported
        self.prev_distance = 0  # ç”¨æ–¼è¨ˆç®—é€²åº¦çå‹µ
        self.timeout = 180  # å–®åœˆè¶…æ™‚æ™‚é–“ï¼ˆç§’ï¼‰- å¢åŠ å®¹è¨±æ™‚é–“
        self.start_time = 0  # è¨˜éŒ„é–‹å§‹æ™‚é–“
        self.episode_count = 0  # å›åˆè¨ˆæ•¸ï¼Œç”¨æ–¼å‹•æ…‹èª¿æ•´çå‹µ
        # è¿½è¸ªä¸Šä¸€å€‹å‹•ä½œï¼Œç”¨æ–¼è¨ˆç®—è½‰å‘å¹³æ»‘åº¦çå‹µ
        self.prev_steering_action = None
        # é€£çºŒè½‰å‘è¨ˆæ•¸å™¨ (ç”¨æ–¼è½‰å‘éä¹…æ‡²ç½°)
        self.continuous_steering_count = 0
        self.sand_stuck_counter = 0   # åœ¨æ²™åœ°ä¸”æ²’é€Ÿåº¦çš„é€£çºŒå¹€æ•¸
        # æ–°å¢çµ±è¨ˆæ•¸æ“š
        self.crash_count = 0  # æ’è»Šæ¬¡æ•¸
        self.speed_history = []  # é€Ÿåº¦æ­·å²è¨˜éŒ„
        self.distance_history = []  # è·é›¢ä¸­å¿ƒçš„æ­·å²è¨˜éŒ„
        # æ–°å¢å¡ä½æª¢æ¸¬
        self.stuck_counter = 0  # å¡ä½è¨ˆæ•¸å™¨
        self.stuck_threshold = 300    # å¡ä½é–¾å€¼ï¼ˆå¹€æ•¸ï¼‰
        self.min_progress_threshold = 0.05  # æœ€å°é€²åº¦é–¾å€¼
        
        # æ–°å¢é‡Œç¨‹ç¢‘è¿½è¸ª
        self.last_milestone = 0  # æœ€å¾Œé”åˆ°çš„é‡Œç¨‹ç¢‘ (0-20) - å› ç‚ºæ”¹ç‚ºæ¯5%ä¸€å€‹é‡Œç¨‹ç¢‘
        self.last_rewarded_lap = 0  # æœ€å¾Œçå‹µçš„åœˆæ•¸
        self.low_speed_time = 0.0  # ä½é€ŸæŒçºŒæ™‚é–“è¨ˆæ•¸å™¨
        self.steps_in_episode = 0  # ç•¶å‰å›åˆçš„æ­¥æ•¸
        
        self.debug = debug
        self.debug_rewards = False
        self.debug_interval = 20  # æ¯éš”20æ­¥è¼¸å‡ºä¸€æ¬¡è¨Šæ¯
        self._in_sand_prev = False

        self.reset_episode_stats()
    
    def _start_game(self, mode="MODE_AI"):
        """å•Ÿå‹•éŠæˆ²é€²ç¨‹ä¸¦è¨­ç½®ç‚ºAIæ¨¡å¼"""
        try:
            # é–‹å•Ÿæ–°çš„å­é€²ç¨‹åŸ·è¡ŒéŠæˆ²ï¼Œä½¿ç”¨start_new_session=Trueç¢ºä¿å³ä½¿çˆ¶é€²ç¨‹è¢«killä¹Ÿä¸æœƒç•™ä¸‹æ®­å±é€²ç¨‹
            if os.name == 'nt':  # Windows
                self.process = subprocess.Popen([self.exe_path, mode], 
                                          stdin=subprocess.PIPE, 
                                          stdout=subprocess.PIPE,
                                          creationflags=subprocess.CREATE_NEW_PROCESS_GROUP)
            else:  # Linux/Mac
                self.process = subprocess.Popen([self.exe_path, mode], 
                                          stdin=subprocess.PIPE, 
                                          stdout=subprocess.PIPE,
                                          start_new_session=True)
            time.sleep(2)  # ç­‰å¾…éŠæˆ²å•Ÿå‹•
            
            # å˜—è©¦å»ºç«‹é€£æ¥
            connected = self.game_interface.connect()
            if not connected:
                print("ç„¡æ³•é€£æ¥åˆ°éŠæˆ²ï¼Œå°‡å˜—è©¦é—œé–‰ä¸¦é‡æ–°å•Ÿå‹•")
                if self.process:
                    self.process.terminate()
                    time.sleep(1)
                return False
            
            return True
        except Exception as e:
            print(f"å•Ÿå‹•éŠæˆ²å¤±æ•—: {e}")
            return False
    
    def reset(self):
        """é‡ç½®ç’°å¢ƒï¼Œé‡æ–°é–‹å§‹è³½è»Š"""
        # æ›´æ–°å›åˆè¨ˆæ•¸
        self.episode_count += 1
        
        # å¦‚æœå·²æœ‰é€£æ¥ï¼Œå…ˆå˜—è©¦ç™¼é€é‡ç½®å‘½ä»¤
        if self.game_interface.connected:
            reset_success = self.game_interface.reset_game()
            if reset_success:
                time.sleep(1.5)  # ç­‰å¾…éŠæˆ²é‡ç½®
                # é‡ç½®å…§éƒ¨ç‹€æ…‹
                self.lap_time = 0
                self.travel_distance = 0
                self.last_checkpoint = 0
                self.lap_count = 0
                self.prev_distance = 0  # ç¢ºä¿prev_distanceé‡ç½®ç‚º0
                self.start_time = time.time()
                self.stuck_counter = 0  # é‡ç½®å¡ä½è¨ˆæ•¸å™¨
                self.last_milestone = 0  # é‡ç½®é‡Œç¨‹ç¢‘è¨ˆæ•¸å™¨
                self.last_rewarded_lap = 0  # é‡ç½®åœˆæ•¸çå‹µè¨ˆæ•¸å™¨
                self.low_speed_time = 0.0  # é‡ç½®ä½é€Ÿè¨ˆæ™‚å™¨
                self.steps_in_episode = 0  # é‡ç½®å›åˆæ­¥æ•¸
                self.prev_steering_action = None  # é‡ç½®å‰ä¸€å€‹è½‰å‘å‹•ä½œ
                self.continuous_steering_count = 0  # é‡ç½®é€£çºŒè½‰å‘è¨ˆæ•¸å™¨
                self.reset_episode_stats()  # é‡ç½®çµ±è¨ˆæ•¸æ“š
                
                # æ¯10å›åˆè¼¸å‡ºä¸€æ¬¡è¨Šæ¯
                if self.debug and self.episode_count % 10 == 0:
                    self.debug_rewards = True
                else:
                    self.debug_rewards = False
                    
                return self.game_interface.get_state()
        
        # å¦‚æœæ²’æœ‰é€£æ¥æˆ–é‡ç½®å¤±æ•—ï¼Œå‰‡é—œé–‰èˆŠé€²ç¨‹ä¸¦å•Ÿå‹•æ–°éŠæˆ²
        if self.process:
            self.process.terminate()
            time.sleep(1)
            
        success = self._start_game()
        if not success:
            raise Exception("ç„¡æ³•å•Ÿå‹•éŠæˆ²ç’°å¢ƒ")
        
        # é‡ç½®å…§éƒ¨ç‹€æ…‹
        self.lap_time = 0
        self.travel_distance = 0
        self.last_checkpoint = 0
        self.lap_count = 0
        self.prev_distance = 0  # ç¢ºä¿prev_distanceé‡ç½®ç‚º0
        self.start_time = time.time()
        self.stuck_counter = 0  # é‡ç½®å¡ä½è¨ˆæ•¸å™¨
        self.last_milestone = 0  # é‡ç½®é‡Œç¨‹ç¢‘è¨ˆæ•¸å™¨
        self.last_rewarded_lap = 0  # é‡ç½®åœˆæ•¸çå‹µè¨ˆæ•¸å™¨ 
        self.low_speed_time = 0.0  # é‡ç½®ä½é€Ÿè¨ˆæ™‚å™¨
        self.steps_in_episode = 0  # é‡ç½®å›åˆæ­¥æ•¸
        self.prev_steering_action = None  # é‡ç½®å‰ä¸€å€‹è½‰å‘å‹•ä½œ
        self.continuous_steering_count = 0  # é‡ç½®é€£çºŒè½‰å‘è¨ˆæ•¸å™¨
        self.reset_episode_stats()  # é‡ç½®çµ±è¨ˆæ•¸æ“š
        
        # ç²å–åˆå§‹ç‹€æ…‹
        time.sleep(1.5)  # çµ¦éŠæˆ²ä¸€é»æ™‚é–“åˆå§‹åŒ–
        return self.game_interface.get_state()

    def _calculate_reward(self, s_prev, s_next, done):
        v_prev = np.clip(s_prev[4], 0., MAX_F_ROAD_CONST) # Speed from previous state
        v_next = np.clip(s_next[4], 0., MAX_F_ROAD_CONST) # Current speed (after action)
        
        d_center_next = abs(s_next[7])
        on_road_next = d_center_next <= ROAD_HALF_W_CONST
        on_sand_next = (ROAD_HALF_W_CONST < d_center_next <= SAND_HALF_W_CONST)
        off_track_next = d_center_next > SAND_HALF_W_CONST

        theta_next = abs(s_next[8]) 
        angle_k_next = 1.0 + (theta_next / (90.0 + 1e-6)) ** ALPHA_SAND_ANGLE

        heading_prev = s_prev[3]
        heading_next = s_next[3]
        heading_diff = heading_next - heading_prev
        if heading_diff > 180: heading_diff -= 360
        elif heading_diff < -180: heading_diff += 360
        abs_heading_change_per_step = abs(heading_diff)

        r = 0.0

        # --- 1. Track Completion & Progress ---
        dp = self.travel_distance - self.prev_distance 
        if dp > 0.01: 
            r += K_PROGRESS * dp 
            # Reward for maintaining angle while progressing (good for exiting turns)
            if theta_next < MAX_ANGLE_ON_ROAD_FOR_PENALTY * 0.75: # If well aligned
                 r += dp * K_ALIGN * 2.0 # Small bonus for progressing straight
        elif dp < -0.005: 
            r += PEN_WRONG_WAY * abs(dp) 

        if self.lap_count > self.last_rewarded_lap:
            r += BONUS_LAP 
            self.last_rewarded_lap = self.lap_count
            print(f"ğŸ‰ Lap {self.lap_count} completed! Bonus: {BONUS_LAP}")

        # --- 2. Speed Management ---
        if on_road_next:
            if v_next > TARGET_SPEED_THRESHOLD:
                speed_excess_ratio = (v_next - TARGET_SPEED_THRESHOLD) / (MAX_F_ROAD_CONST - TARGET_SPEED_THRESHOLD + 1e-6)
                r += K_HIGH_SPEED_BONUS * np.clip(speed_excess_ratio, 0.0, 1.0)
                if v_next > MAX_F_ROAD_CONST * 0.9:
                    r += K_SPEED_MAINTAIN * (v_next / (MAX_F_ROAD_CONST + 1e-6))
            elif v_next < (TARGET_SPEED_THRESHOLD * 0.5):
                r += PEN_VERY_LOW_SPEED 
            elif v_next < (TARGET_SPEED_THRESHOLD * 0.8):
                r += PEN_LOW_SPEED
        else: # Off-road - harsher speed penalty
            if v_next > MAX_F_SAND_CONST * 0.1:
                 r -= K_HIGH_SPEED_BONUS * 1.2 * (v_next / (MAX_F_SAND_CONST + 1e-6))

        # --- 3. Avoid Sand & Off-Track ---
        if on_sand_next:
            if not self._in_sand_prev: # Just hit sand (self._in_sand_prev was updated at end of previous step)
                r += PEN_SAND_HIT 
                print(f"ğŸ’¥ Hit Sand! Penalty: {PEN_SAND_HIT:.2f}")
            r += PEN_SAND_EACH_FR * angle_k_next
        
        if off_track_next:
            r += PEN_OFF_TRACK * angle_k_next
            print(f"ğŸï¸ Off Track! Penalty: {PEN_OFF_TRACK * angle_k_next:.2f}")

        # --- 4. Improving Turning Behavior ---
        if on_road_next:
            # Penalize large angle with track (being sideways)
            if theta_next > MAX_ANGLE_ON_ROAD_FOR_PENALTY:
                penalty_factor = ((theta_next - MAX_ANGLE_ON_ROAD_FOR_PENALTY) / 
                                  (90.0 - MAX_ANGLE_ON_ROAD_FOR_PENALTY + 1e-6))
                if d_center_next > ROAD_HALF_W_CONST * 0.7 or dp < 0.02: # Wider margin for being off-center, or very slow progress
                    penalty_factor *= 2.0 # Stronger penalty if also drifting wide or stuck
                r += PEN_LARGE_ANGLE_ON_ROAD * penalty_factor
            
            # Penalize excessive steering change (wiggling) if speed is somewhat high
            if v_next > MIN_SPEED_FOR_STEERING_PENALTY and abs_heading_change_per_step > MAX_HEADING_CHANGE_BEFORE_PENALTY:
                r += PEN_EXCESSIVE_STEERING_CHANGE * (abs_heading_change_per_step / (30.0 + 1e-6)) # Normalized by a smaller divisor

            # NEW: Penalty for sudden speed drop near edges (potential wall hit)
            # This is an approximation for hitting a wall/barrier
            is_near_edge = (d_center_next > ROAD_HALF_W_CONST * 0.8) # If close to edge of road
            sudden_speed_drop = (v_prev > v_next * 1.5) and (v_prev > MIN_SPEED_FOR_STEERING_PENALTY) # Significant speed drop
            if on_road_next and is_near_edge and sudden_speed_drop and dp < 0.1 : # if also not making progress
                r += PEN_WALL_HIT_IMPACT
                print(f"ğŸ§± Possible Wall Hit! Penalty: {PEN_WALL_HIT_IMPACT}")
        
        # --- 5. Fine-tuning / Stability Rewards (Keep very small) ---
        if on_road_next:
            r_center = K_CENTER * (1.0 - (d_center_next / (ROAD_HALF_W_CONST + 1e-6)) ** CENTER_POW)
            r += r_center
            r += K_ALIGN * (1.0 - theta_next / (90.0 + 1e-6))
            r += ON_ROAD_BONUS
        
        self._in_sand_prev = on_sand_next or off_track_next
        self.prev_distance = self.travel_distance
        return r


    def _sanitize_state(self, s):
        """æŠŠ NaN/Inf è½‰æˆæœ‰é™å€¼ï¼Œé¿å…è¨“ç·´æ•´æ®µ crashã€‚"""
        # æŠŠ NaN â†’ 0ï¼Œ+Inf â†’ å¤§æ•¸ï¼Œ-Inf â†’ å°æ•¸
        s = np.nan_to_num(s, nan=0.0, posinf=1e6, neginf=-1e6)
        # é™åˆ¶æ¯å€‹æ¬„ä½çš„åˆç†ç¯„åœï¼Œé˜²æ­¢æ¥µç«¯å€¼å®³æ­»ç¶²è·¯
        # x, z åº§æ¨™
        s[0] = np.clip(s[0], -1000, 1000)
        s[2] = np.clip(s[2], -1000, 1000)
        # speed
        s[4] = np.clip(s[4], -1.5*MAX_F_SAND_CONST, 1.5*MAX_F_ROAD_CONST)
        # èˆ‡ä¸­å¿ƒè·é›¢
        s[7] = np.clip(s[7], -SAND_HALF_W_CONST*1.5,  SAND_HALF_W_CONST*1.5)
        # â€¦è¦–éœ€è¦å†åŠ æ¬„ä½
        return s

    def step(self, action):
        self.steps_in_episode += 1
        pre_action_travel_distance = self.travel_distance
        
        # s_prev for _calculate_reward is current_state_in_step
        s_prev_for_reward = self.game_interface.get_state() # This is the state before the action is taken

        self.game_interface.send_action(action)
        time.sleep(0.04)

        next_state_raw = self.game_interface.get_state() # This is s_next
        next_state = self._sanitize_state(next_state_raw)

        # Update lap info after getting next_state
        self.lap_count, self.lap_time, self.travel_distance, self.last_checkpoint = self.game_interface.get_lap_info()

        if self.total_samples is None or self.last_checkpoint > self.total_samples:
            self.total_samples = self.last_checkpoint

        self.speed_history.append(next_state[4])
        self.distance_history.append(next_state[7])

        # Pass current_state_in_step as s_prev
        reward = self._calculate_reward(s_prev_for_reward, next_state, False)
        
        done = False
        crashed = False
        stuck = False

        if self.lap_count >= 1:
            done = True
            print(f"ğŸ Episode finished: Lap Completed! Raw Reward: {reward:.2f}")
        
        if not done:
            current_step_progress = self.travel_distance - pre_action_travel_distance
            if current_step_progress < self.min_progress_threshold and next_state[4] < 10.0:
                self.stuck_counter += 1
            else:
                self.stuck_counter = 0
            
            if self.stuck_counter >= self.stuck_threshold:
                done = True
                stuck = True
                reward += -800.0  # Further increase override penalty for being properly stuck
                print(f"ğŸš« Agent Stuck! Reward overridden to: {reward:.2f}")

        if not done:
            is_off_main_sand_area = abs(next_state[7]) > SAND_HALF_W_CONST # Use self.
            is_timeout = (time.time() - self.start_time) > self.timeout
            if is_off_main_sand_area or is_timeout:
                done = True
                crashed = True
                self.crash_count += 1
                reason = "OffTrack" if is_off_main_sand_area else "Timeout"
                reward += -1000.0 # Further increase override penalty
                print(f"ğŸ’¥ Agent Crashed ({reason})! Reward overridden to: {reward:.2f}")
        
        if not done:
            if not np.isfinite(next_state).all():
                bad_indices = np.where(np.isnan(next_state) | np.isinf(next_state))[0]
                print(f"[DEBUG] Non-finite state: indices={bad_indices}, vals={next_state[bad_indices]}")
                done = True
                crashed = True
                reward += -700.0
                print(f"ğŸ’£ NaN State! Reward overridden to: {reward:.2f}")

        if not done:
            is_on_sand_for_stuck_check = (ROAD_HALF_W_CONST < abs(next_state[7]) <= SAND_HALF_W_CONST) # Use self.
            if is_on_sand_for_stuck_check and next_state[4] < 5.0:
                self.sand_stuck_counter +=1
            else:
                self.sand_stuck_counter = 0
            
            if self.sand_stuck_counter >= 60: 
                done = True
                crashed = True
                reward += -400.0 
                print(f"â³ Stuck in Sand! Additional penalty. Reward now: {reward:.2f}")
        
        assert np.isfinite(next_state).all(), f"State NaN/Inf: {next_state}"
        assert action in ACTION_TABLE, f"Invalid action: {action}" # ACTION_TABLE is global
        assert math.isfinite(reward), f"Reward NaN/Inf: {reward}"

        return next_state, reward, done, {
            'lap_count': self.lap_count, 'lap_time': self.lap_time,
            'travel_distance': self.travel_distance, 'checkpoint': self.last_checkpoint,
            'crashed': crashed, 'stuck': stuck
        }

    def reset_episode_stats(self):
        """é‡ç½®æ¯å€‹episodeçš„çµ±è¨ˆæ•¸æ“š"""
        self.speed_history = []
        self.distance_history = []
        self.sand_stuck_counter = 0
    def get_episode_stats(self):
        """ç²å–ç•¶å‰episodeçš„çµ±è¨ˆæ•¸æ“š"""
        # è»Šé€Ÿåˆ†å¸ƒçµ±è¨ˆ
        speeds = np.abs(np.array(self.speed_history))
        speed_stats = {
            'mean_speed': np.mean(speeds) if len(speeds) > 0 else 0,
            'max_speed': np.max(speeds) if len(speeds) > 0 else 0,
            'min_speed': np.min(speeds) if len(speeds) > 0 else 0,
            'speed_std': np.std(speeds) if len(speeds) > 0 else 0,
        }
        bins = [0, 30, 60, 90, float('inf')]
        hist, _ = np.histogram(speeds, bins=bins)
        speed_dist = hist / hist.sum() if hist.sum() else np.zeros(4)

        return {
            'crash_count'      : self.crash_count,
            'speed_stats'      : {
                k: float(v) for k, v in speed_stats.items()   # è½‰æˆ float
            },
            'speed_distribution': speed_dist,
        }
        
    def close(self):
        """é—œé–‰ç’°å¢ƒå’ŒéŠæˆ²é€²ç¨‹"""
        if self.game_interface.connected:
            self.game_interface.disconnect()
            
        if self.process:
            try:
                # å˜—è©¦æ­£å¸¸çµ‚æ­¢é€²ç¨‹
                self.process.terminate()
                
                # çµ¦é€²ç¨‹ä¸€äº›æ™‚é–“æ­£å¸¸é—œé–‰
                for _ in range(5):  # å˜—è©¦æœ€å¤š5æ¬¡ï¼Œæ¯æ¬¡ç­‰å¾…0.5ç§’
                    if self.process.poll() is not None:  # å¦‚æœé€²ç¨‹å·²é€€å‡º
                        break
                    time.sleep(0.5)
                
                # å¦‚æœé€²ç¨‹é‚„æ²’é€€å‡ºï¼Œå¼·åˆ¶é—œé–‰
                if self.process.poll() is None:
                    print("éŠæˆ²é€²ç¨‹æœªæ­£å¸¸çµ‚æ­¢ï¼Œå˜—è©¦å¼·åˆ¶é—œé–‰")
                    if os.name == 'nt':  # Windows
                        subprocess.run(['taskkill', '/F', '/T', '/PID', str(self.process.pid)], 
                                      stderr=subprocess.PIPE, stdout=subprocess.PIPE)
                    else:  # Linux/Mac
                        import signal
                        os.killpg(os.getpgid(self.process.pid), signal.SIGKILL)
            except Exception as e:
                print(f"é—œé–‰éŠæˆ²é€²ç¨‹æ™‚å‡ºéŒ¯: {e}")
            finally:
                self.process = None

# è¨“ç·´å‡½æ•¸
def train_agent(agent, env, num_episodes, max_steps_per_episode=3000, 
                save_freq=100, model_dir="models", log_interval=1,
                warmup_steps=5000, forward_pretraining=True, pretraining_episodes=30,
                turn_training_episodes=10):
    
    os.makedirs(model_dir, exist_ok=True)
    
    # å‰µå»ºlogsç›®éŒ„
    log_dir = os.path.join(model_dir, 'logs')
    os.makedirs(log_dir, exist_ok=True)

    # åˆå§‹åŒ–CSVè¨˜éŒ„å™¨
    csv_log_path = os.path.join(log_dir, f"training_log_{agent.__class__.__name__}_{time.strftime('%Y%m%d-%H%M%S')}.csv")
    csv_fields = ['episode', 'steps', 'total_reward', 'avg_reward', 'epsilon', 'loss', 'mean_q', 'mean_speed', 
                 'crash_count', 'completed']
    
    
    # å‰µå»ºCSVæ–‡ä»¶ä¸¦å¯«å…¥æ¬„ä½åç¨±
    with open(csv_log_path, 'w', newline='', encoding='utf-8-sig') as csvfile:
        writer_csv = csv.DictWriter(csvfile, fieldnames=csv_fields)
        writer_csv.writeheader()
    
    total_rewards = []
    avg_rewards = []
    
    # æ–°å¢ç›£æ§æŒ‡æ¨™
    mean_q_values = []
    crash_counts = []
    completed_laps = []
    mean_speeds = []
    
    # æ–°å¢å®Œæˆç™¾åˆ†æ¯”è¿½è¸ª
    completion_percentages = []
    # é€Ÿåº¦åˆ†å¸ƒç´¯ç©æ•¸æ“š (0-30, 30-60, 60-90, 90+)
    speed_distribution = np.zeros(4)
    speed_dist_counts = 0
    
    
    # å„²å­˜åŸå§‹epsilonè¨­å®šï¼Œç”¨æ–¼å¾ŒçºŒèª¿æ•´
    original_epsilon_min = agent.epsilon_min
    
    # é€²è¡ŒReplay Bufferé ç†±
    if warmup_steps > 0:
        print(f"é–‹å§‹Replay Bufferé ç†±ï¼ŒåŸ·è¡Œ {warmup_steps} æ­¥éš¨æ©Ÿå‹•ä½œ...")
        state = env.reset()
        original_epsilon = agent.epsilon  # ä¿å­˜åŸå§‹epsilon
        agent.epsilon = 1.0  # ç¢ºä¿å®Œå…¨éš¨æ©Ÿ
        
        for step in range(warmup_steps):
            # é¸æ“‡éš¨æ©Ÿå‹•ä½œ
            action_game, action_id = agent.act(state)

            
            # åŸ·è¡Œå‹•ä½œ
            next_state, reward, done, info = env.step(action_game)
            
            # è¨˜æ†¶ç¶“é©—ï¼Œä½†ä¸è¨“ç·´
            agent.remember(state, action_id, reward, next_state, done)
            
            # æ›´æ–°ç‹€æ…‹
            state = next_state
            
            if done:
                state = env.reset()
                print(f"é ç†±é‡ç½®: æ­¥æ•¸ {step+1}/{warmup_steps}")
        
        # æ¢å¾©åŸå§‹epsilonå€¼
        agent.epsilon = original_epsilon
        print("Replay Bufferé ç†±å®Œæˆ")
    
    # å‰é€²å‹•ä½œé è¨“ç·´ (åªå…è¨±å‘å‰/éœæ­¢å‹•ä½œ)
    if forward_pretraining and pretraining_episodes > 0:
        print(f"é–‹å§‹å‰é€²å‹•ä½œé è¨“ç·´ï¼ŒåŸ·è¡Œ {pretraining_episodes} å€‹å›åˆ...")
        
        # ä¿å­˜åŸå§‹epsilonå€¼ç”¨æ–¼å¾ŒçºŒæ¢å¾©
        original_epsilon = agent.epsilon
        
        # è¨­ç½®è¼ƒé«˜çš„æ¢ç´¢ç‡ä»¥é¼“å‹µå˜—è©¦å‰é€²å‹•ä½œ
        agent.epsilon = 0.7
        
        # å•Ÿç”¨å‰é€²é è¨“ç·´æ¨¡å¼
        agent.forward_pretraining_mode = True
        
        for episode in range(1, pretraining_episodes + 1):
            # é‡ç½®ç’°å¢ƒ
            state = env.reset()
            done = False
            total_reward = 0
            steps = 0
            
            while not done and steps < max_steps_per_episode:
                # é¸æ“‡å‹•ä½œ - åªæœƒæ˜¯å‰é€²(W)æˆ–éœæ­¢(idle)
                action_game, action_id = agent.act(state, training=True)
                
                # åŸ·è¡Œå‹•ä½œ
                next_state, reward, done, info = env.step(action_game)
                
                # è¨˜æ†¶ç¶“é©—
                agent.remember(state, action_id, reward, next_state, done)
                
                # è¨“ç·´ç¶²çµ¡
                agent.train()
                
                # æ›´æ–°ç‹€æ…‹
                state = next_state
                total_reward += reward
                steps += 1
                
                if done:
                    break
            
            print(f"å‰é€²é è¨“ç·´ - Episode {episode}/{pretraining_episodes}, æ­¥æ•¸: {steps}, çå‹µ: {total_reward:.2f}")
        
        # æ¢å¾©åŸå§‹epsilonå€¼ä¸¦é—œé–‰å‰é€²é è¨“ç·´æ¨¡å¼
        agent.epsilon = original_epsilon
        agent.forward_pretraining_mode = False
        print("å‰é€²å‹•ä½œé è¨“ç·´å®Œæˆ")
    
    # ä¸»è¦è¨“ç·´éšæ®µ
    print(f"é–‹å§‹ä¸»è¦è¨“ç·´éšæ®µï¼ŒåŸ·è¡Œ {num_episodes} å€‹å›åˆ...")
    
    # é‡ç½®ç’°å¢ƒç‹€æ…‹
    env.reset_episode_stats()
    agent.reset_episode_stats()
    
    # è¨˜éŒ„è¨“ç·´é–‹å§‹æ™‚é–“
    start_training_time = time.time()
    
    # åœˆæ•¸å®Œæˆé€šçŸ¥è¨ˆæ•¸
    lap_completion_count = 0
    
    for episode in range(1, num_episodes + 1):
        episode_start_time = time.time()
        
        # é‡ç½®ç’°å¢ƒï¼Œç²å–åˆå§‹ç‹€æ…‹
        state = env.reset()
        
        # æª¢æŸ¥åˆå§‹ç‹€æ…‹æ˜¯å¦æœ‰æ•ˆï¼ˆé¿å…è² é€Ÿåº¦æˆ–æ¥µç«¯è§’åº¦æ–¹å‘çš„æƒ…æ³ï¼‰
        invalid_state_counter = 0
        max_reset_attempts = 3
        
        while (state[4] < 0 or abs(state[8]) > 120) and invalid_state_counter < max_reset_attempts:
            print(f"æª¢æ¸¬åˆ°ç„¡æ•ˆåˆå§‹ç‹€æ…‹: é€Ÿåº¦={state[4]:.2f}, è§’åº¦={state[8]:.2f}. å˜—è©¦é‡ç½®...")
            state = env.reset()
            invalid_state_counter += 1
            
        if invalid_state_counter == max_reset_attempts:
            print("è­¦å‘Š: å¤šæ¬¡é‡ç½®å¾Œä»ç„¶æœ‰ç„¡æ•ˆç‹€æ…‹ï¼Œç¹¼çºŒåŸ·è¡Œä½†å¯èƒ½å½±éŸ¿è¨“ç·´")
        
        total_reward = 0
        losses = []
        done = False
        
        # é‡ç½®æ¯å€‹episodeçš„çµ±è¨ˆ
        env.reset_episode_stats()
        agent.reset_episode_stats()
        
        for step in range(max_steps_per_episode):
            # é¸æ“‡å‹•ä½œ
            action_game, action_id = agent.act(state)
                    
            # åŸ·è¡Œå‹•ä½œä¸¦ç²å–ä¸‹ä¸€å€‹ç‹€æ…‹ã€çå‹µç­‰è¨Šæ¯
            next_state, reward, done, info = env.step(action_game)
            
            # å„²å­˜ç¶“é©—
            agent.remember(state, action_id, reward, next_state, done)
            
            
            # è¨“ç·´ç¶²çµ¡
            loss = agent.train()
            if loss > 0:
                losses.append(loss)
            
            # æ›´æ–°ç•¶å‰ç‹€æ…‹å’Œç¸½çå‹µ
            state = next_state
            total_reward += reward
            # å¦‚æœå›åˆçµæŸï¼Œå‰‡è·³å‡ºå¾ªç’°
            if done:
                break
                
        # å¿½ç•¥éçŸ­çš„å›åˆï¼ˆå¯èƒ½æ˜¯ç’°å¢ƒåˆå§‹åŒ–éŒ¯èª¤ï¼‰
        if step < 30:
            print(f"å›åˆ {episode} éçŸ­ (åªæœ‰ {step} æ­¥)ï¼Œå¯èƒ½æ˜¯ç’°å¢ƒéŒ¯èª¤ - å¿½ç•¥çµ±è¨ˆ")
            continue
        
        # è¨ˆç®—æœ¬å›åˆç”¨æ™‚
        episode_time = time.time() - episode_start_time
        
        # æ›´æ–°epsilon
        agent.decay_epsilon()
        
        # è¨ˆç®—å¹³å‡æå¤±
        avg_loss = np.mean(losses) if len(losses) > 0 else 0
        
        # ç²å–æœ¬å›åˆçš„çµ±è¨ˆæ•¸æ“š
        env_stats = env.get_episode_stats()
        agent_stats = agent.get_episode_stats()
        
        # è¨˜éŒ„æœ¬å›åˆæ˜¯å¦æ’è»Šæˆ–å®Œæˆè³½é“
        episode_crashed = info.get('crashed', False)
        episode_completed = info.get('lap_count', 0) > 0
        
        # æª¢æŸ¥æ˜¯å¦å®Œæˆäº†ä¸€åœˆï¼Œå¦‚æœå®Œæˆå‰‡ä¿å­˜æ¨¡å‹å¿«ç…§ä¸¦ç™¼é€é€šçŸ¥
        if episode_completed:
            # ä¿å­˜å®Œæˆä¸€åœˆæ™‚çš„æ¨¡å‹å’Œç¶“é©—å›æ”¾å¿«ç…§
            lap_model_path = os.path.join(model_dir, f"lap_completed_model_ep{episode}.pth")
            agent.save(lap_model_path)
            print(f"å·²ä¿å­˜å®Œæˆä¸€åœˆçš„æ¨¡å‹å¿«ç…§: {lap_model_path}")
            
            if email_config['enabled'] and email_config['notify_lap_completion']:
                lap_completion_count += 1
            # æº–å‚™åœ–è¡¨ä½œç‚ºé™„ä»¶
            plot_path = os.path.join(model_dir, f"lap_completion_{lap_completion_count}.png")
            plot_training_progress(episode, total_rewards, avg_rewards, mean_q_values, 
                                    crash_counts, completed_laps, mean_speeds, 
                                    speed_distribution, completion_percentages, save_dir=model_dir)
            
            # æº–å‚™é€šçŸ¥å…§å®¹
            subject = f"è³½è»Šè¨“ç·´é€šçŸ¥: å®Œæˆä¸€åœˆ! (ç¬¬{lap_completion_count}æ¬¡)"
            message = (f"æ¨¡å‹åœ¨è¨“ç·´éç¨‹ä¸­æˆåŠŸå®Œæˆäº†ä¸€åœˆ!\n\n"
                      f"å®Œæˆæ™‚é–“: {time.strftime('%Y-%m-%d %H:%M:%S')}\n"
                      f"è¨“ç·´å›åˆ: {episode}/{num_episodes}\n"
                      f"å–®åœˆæ™‚é–“: {info['lap_time']:.2f}ç§’\n"
                      f"å–®åœˆè·é›¢: {info['travel_distance']:.2f}\n"
                      f"ç¸½çå‹µ: {total_reward:.2f}\n"
                      f"å¹³å‡é€Ÿåº¦: {env_stats['speed_stats']['mean_speed']:.2f}\n\n"
                      f"é€™æ˜¯æ¨¡å‹ç¬¬{lap_completion_count}æ¬¡å®Œæˆä¸€åœˆã€‚")
            
            # ç™¼é€é€šçŸ¥
            send_email_notification(
                subject=subject,
                message=message,
                attachments=[("training_progress.png", plot_path)]
            )
        
        # æ›´æ–°ç›£æ§æŒ‡æ¨™
        total_rewards.append(total_reward)
        
        # è¨ˆç®—ç§»å‹•å¹³å‡çå‹µ
        window = min(100, len(total_rewards))
        avg_reward = np.mean(total_rewards[-window:])
        avg_rewards.append(avg_reward)
        
        # ä¿å­˜çµ±è¨ˆæ•¸æ“š
        mean_q_values.append(agent_stats['avg_q'])
        crash_counts.append(1 if episode_crashed else 0)
        completed_laps.append(1 if episode_completed else 0)
        mean_speeds.append(env_stats['speed_stats']['mean_speed'])
        
        # è¨ˆç®—ä¸¦è¨˜éŒ„å®Œæˆç™¾åˆ†æ¯”
        track_samples_size = 1963  
        completion_percentage = min(100, (info['travel_distance'] / track_samples_size) * 100)
        completion_percentages.append(completion_percentage)
        
        # æ ¹æ“šå®Œæˆç‡èª¿æ•´epsilon (å¦‚æœå®Œæˆç‡>70%ï¼Œå°‡epsilonè¨­ç‚º0.01)
        if completion_percentage > 70 and agent.epsilon > 0.01:
            old_epsilon = agent.epsilon
            agent.epsilon = 0.01
            print(f"å› å®Œæˆç‡é”{completion_percentage:.1f}%ï¼Œå°‡epsilonå¾{old_epsilon:.4f}é™è‡³0.01")
        
        # æ›´æ–°é€Ÿåº¦åˆ†å¸ƒçµ±è¨ˆ
        if speed_dist_counts == 0:
            speed_distribution = env_stats['speed_distribution']
        else:
            speed_distribution = (speed_distribution * speed_dist_counts + env_stats['speed_distribution']) / (speed_dist_counts + 1)
        speed_dist_counts += 1
        
        # å®šæœŸä¿å­˜æ¨¡å‹
        if episode % save_freq == 0 or episode == num_episodes:
            save_path = os.path.join(model_dir, f"agent_ep{episode}.pth")
            agent.save(save_path)
            
            # ç¹ªè£½è¨“ç·´é€²åº¦åœ–
            plot_training_progress(episode, total_rewards, avg_rewards, mean_q_values, 
                                      crash_counts, completed_laps, mean_speeds, 
                                      speed_distribution, completion_percentages, save_dir=model_dir)
            
        
        # æ‰“å°è¨“ç·´è¨Šæ¯
        if episode % log_interval == 0:
            # æ ¼å¼åŒ–è¼¸å‡ºè¨“ç·´è¨Šæ¯
            print(f"Episode {episode}/{num_episodes} - "
                  f"Steps: {step+1}, "
                  f"Total Reward: {total_reward:.2f}, "
                  f"Avg Reward: {avg_reward:.2f}, "
                  f"Epsilon: {agent.epsilon:.4f}, "
                  f"Loss: {avg_loss:.6f}\n"
                  f"Mean Q-value: {agent_stats['avg_q']:.2f}, "
                  f"Mean Speed: {env_stats['speed_stats']['mean_speed']:.2f}, "
                  f"Crash Count (total): {env.crash_count}, "
                  f"Travel Distance: {info['travel_distance']:.2f}, "
                  f"Time: {episode_time:.2f}s")
            
            
            # å¦‚æœæ­¤å›åˆå®Œæˆäº†ä¸€åœˆ
            if episode_completed:
                print(f"æˆåŠŸå®Œæˆè³½é“! å–®åœˆæ™‚é–“: {info['lap_time']:.2f}s, å–®åœˆè·é›¢: {info['travel_distance']:.2f}")
            else:
                # å¦‚æœæ²’æœ‰å®Œæˆï¼Œé¡¯ç¤ºå®Œæˆç™¾åˆ†æ¯”
                if 'travel_distance' in info:
                    # ä½¿ç”¨checkpointç´¢å¼•è¨ˆç®—å®Œæˆç™¾åˆ†æ¯”
                    track_samples_size = 1963  # è»Œé“æ¨£æœ¬ç¸½æ•¸
                    completion_percentage = min(100, (info['travel_distance'] / track_samples_size) * 100)
                    print(f"è³½é“å®Œæˆåº¦: {completion_percentage:.1f}%")
        
        
        # è¨˜éŒ„å®Œæˆç™¾åˆ†æ¯”
        if not episode_completed and 'travel_distance' in info:
            track_samples_size = 1963  # èˆ‡ä¸Šé¢ä¿æŒä¸€è‡´
            completion_percentage = min(100, (info['travel_distance'] / track_samples_size) * 100)
        
        # è¨˜éŒ„åˆ°CSVæ–‡ä»¶
        csv_data = {
            'episode': episode,
            'steps': step+1,
            'total_reward': total_reward,
            'avg_reward': avg_reward,
            'epsilon': agent.epsilon,
            'loss': avg_loss,
            'mean_q': agent_stats['avg_q'],
            'mean_speed': env_stats['speed_stats']['mean_speed'],
            'crash_count': env.crash_count,
            'completed': 1 if episode_completed else 0
        }  
        with open(csv_log_path, 'a', newline='', encoding='utf-8-sig') as csvfile:
            writer_csv = csv.DictWriter(csvfile, fieldnames=csv_fields)
            writer_csv.writerow(csv_data)
    
    # ç¸½è¨“ç·´æ™‚é–“
    total_training_time = time.time() - start_training_time
    print(f"ç¸½è¨“ç·´æ™‚é–“: {total_training_time:.2f}s, å¹³å‡æ¯å€‹å›åˆ: {total_training_time/num_episodes:.2f}s")
    
    # ç¹ªè£½æœ€çµ‚è¨“ç·´é€²åº¦åœ–
    plot_training_progress(num_episodes, total_rewards, avg_rewards, mean_q_values, 
                              crash_counts, completed_laps, mean_speeds, 
                              speed_distribution, completion_percentages, save_dir=model_dir, final=True)

    # è¨˜éŒ„è¨“ç·´æ‘˜è¦
    summary_path = os.path.join(log_dir, f"training_summary_{agent.__class__.__name__}_{time.strftime('%Y%m%d-%H%M%S')}.txt")
    with open(summary_path, 'w', encoding='utf-8-sig') as f:
        f.write(f"è¨“ç·´æ‘˜è¦ - {agent.__class__.__name__}\n")
        f.write(f"è¨“ç·´æ™‚é–“: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"ç¸½å›åˆæ•¸: {num_episodes}\n")
        f.write(f"ç¸½è¨“ç·´æ™‚é–“: {total_training_time:.2f}ç§’\n")
        f.write(f"å¹³å‡æ¯å›åˆæ™‚é–“: {total_training_time/num_episodes:.2f}ç§’\n")
        f.write(f"æœ€çµ‚çå‹µ: {total_rewards[-1]:.2f}\n")
        f.write(f"æœ€çµ‚å¹³å‡çå‹µ: {avg_rewards[-1]:.2f}\n")
        f.write(f"ç¸½æ’è»Šæ¬¡æ•¸: {sum(crash_counts)}\n")
        f.write(f"ç¸½å®Œæˆåœˆæ•¸: {sum(completed_laps)}\n")
        f.write(f"å®Œæˆç‡: {sum(completed_laps)/num_episodes*100:.2f}%\n")
        f.write(f"å¹³å‡é€Ÿåº¦: {np.mean(mean_speeds):.2f}\n")
        f.write(f"æœ€çµ‚Qå€¼: {mean_q_values[-1]:.2f}\n")
        f.write("\né€Ÿåº¦åˆ†å¸ƒ:\n")
        for i, speed_bin in enumerate(['0-30', '30-60', '60-90', '90+']):
            f.write(f"  {speed_bin}: {speed_distribution[i]*100:.2f}%\n")
    print(f"è¨“ç·´æ‘˜è¦å·²ä¿å­˜è‡³: {summary_path}")
    
    # ç™¼é€è¨“ç·´å®Œæˆé€šçŸ¥
    if email_config['enabled'] and email_config['notify_training_completion']:
        subject = "è³½è»Šè¨“ç·´é€šçŸ¥: è¨“ç·´å·²å®Œæˆ"
        message = (f"è³½è»Šè¨“ç·´å·²å®Œæˆ!\n\n"
                  f"å®Œæˆæ™‚é–“: {time.strftime('%Y-%m-%d %H:%M:%S')}\n"
                  f"ç¸½å›åˆæ•¸: {num_episodes}\n"
                  f"ç¸½è¨“ç·´æ™‚é–“: {total_training_time:.2f}ç§’\n"
                  f"å¹³å‡æ¯å›åˆæ™‚é–“: {total_training_time/num_episodes:.2f}ç§’\n"
                  f"æœ€çµ‚çå‹µ: {total_rewards[-1]:.2f}\n")
        
        # ç™¼é€å¸¶é™„ä»¶çš„éƒµä»¶
        # After the final plot_training_progress call
        final_plot_path = os.path.join(model_dir, "final_training_stats.png")
        send_email_notification(
            subject=subject,
            message=message,
            attachments=[
                ("final_training_stats.png", final_plot_path),
                ("training_summary.txt", summary_path)
            ]
        )
    
    return total_rewards, avg_rewards

def plot_rewards(rewards, avg_rewards=None, window=100):
    plt.figure(figsize=(12, 6))
    plt.plot(rewards, label='Episode Reward', alpha=0.6)
    
    if avg_rewards is None and len(rewards) >= window:
        # è¨ˆç®—ç§»å‹•å¹³å‡
        avg_rewards = [np.mean(rewards[max(0, i-window):i+1]) for i in range(len(rewards))]
        plt.plot(avg_rewards, label=f'Moving Average ({window} ep)', color='red')
    elif avg_rewards is not None:
        plt.plot(range(len(avg_rewards)), avg_rewards, label='Average Reward', color='red')
    
    plt.xlabel('Episode')
    plt.ylabel('Reward')
    plt.title('Training Progress')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.savefig("training_progress.png")
    plt.show()

def plot_training_progress(episode, rewards, avg_rewards, q_values, crashes, completions, 
                          speeds, speed_dist, completion_percentages=None, save_dir="models", final=False):
    """ç¹ªè£½å®Œæ•´çš„è¨“ç·´é€²åº¦åœ–è¡¨ï¼ŒåŒ…å«æ‰€æœ‰ç›£æ§æŒ‡æ¨™"""
    plt.style.use('ggplot')
    
    # ç¢ºä¿æ•¸æ“šæ˜¯åˆ—è¡¨è€Œétensor
    if isinstance(rewards, torch.Tensor):
        rewards = rewards.tolist()
    if isinstance(avg_rewards, torch.Tensor):
        avg_rewards = avg_rewards.tolist()
    if isinstance(q_values, torch.Tensor):
        q_values = q_values.tolist()
    if isinstance(crashes, torch.Tensor):
        crashes = crashes.tolist()
    if isinstance(completions, torch.Tensor):
        completions = completions.tolist()
    if isinstance(speeds, torch.Tensor):
        speeds = speeds.tolist()
    if isinstance(completion_percentages, torch.Tensor):
        completion_percentages = completion_percentages.tolist()
    
    # æª¢æŸ¥é€Ÿåº¦å€¼æ˜¯å¦ç‚ºæœ‰æ•ˆæ•¸å€¼ï¼Œæ›´åš´æ ¼çš„æª¢æŸ¥
    valid_speeds = False
    if speeds and len(speeds) > 0:
        # ç§»é™¤éæ•¸å€¼å…ƒç´ å’ŒNaNå€¼
        filtered_speeds = []
        for s in speeds:
            if isinstance(s, numbers.Real) and not (np.isnan(s) or np.isinf(s)):
                filtered_speeds.append(float(s))
        
        # æ›´æ–°speedsç‚ºéæ¿¾å¾Œçš„æ•¸æ“š
        speeds = filtered_speeds
        valid_speeds = len(speeds) > 0
        
        # ç¢ºä¿æ•¸å€¼åˆç†ï¼ˆå»é™¤ç•°å¸¸å¤§æˆ–å°çš„å€¼ï¼‰
        if valid_speeds:
            # æª¢æŸ¥é€Ÿåº¦ç¯„åœï¼Œå‰”é™¤ä¸åˆç†çš„é€Ÿåº¦å€¼
            speeds = [s for s in speeds if 0 <= s <= 150]  # å‡è¨­é€Ÿåº¦ç¯„åœåœ¨0-150ä¹‹é–“
            valid_speeds = len(speeds) > 0
    

    

            

    fig = plt.figure(figsize=(20, 20))  # å¢åŠ é«˜åº¦
    total_rows = 4  # 4è¡Œå­åœ– (å¢åŠ åˆ°4è¡Œä¾†å®¹ç´å®Œæˆç™¾åˆ†æ¯”)
    
    # 1. çå‹µæ›²ç·š
    ax1 = fig.add_subplot(total_rows, 2, 1)
    if rewards and len(rewards) > 0:
        ax1.plot(rewards, 'b-', alpha=0.5, label='Episode Reward')
    if avg_rewards and len(avg_rewards) > 0:
        ax1.plot(avg_rewards, 'r-', label='Avg Reward (100 ep)')
    ax1.set_xlabel('Episode')
    ax1.set_ylabel('Reward')
    ax1.set_title('Training Rewards')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # 2. Qå€¼è®ŠåŒ–
    ax2 = fig.add_subplot(total_rows, 2, 2)
    if q_values and len(q_values) > 0 and all(isinstance(x, (int, float)) for x in q_values if x is not None):
        # éæ¿¾Noneå’ŒNaNå€¼
        valid_q_values = [x for x in q_values if x is not None and not (isinstance(x, float) and np.isnan(x))]
        if valid_q_values:
            ax2.plot(valid_q_values, 'g-', label='Mean Q-Value')
            # è¨ˆç®—Qå€¼çš„ç§»å‹•å¹³å‡
            window = min(100, len(valid_q_values))
            if window > 0:
                q_moving_avg = [np.mean(valid_q_values[max(0, i-window):i+1]) for i in range(len(valid_q_values))]
                ax2.plot(q_moving_avg, 'r-', label='Moving Avg')
            ax2.set_xlabel('Episode')
            ax2.set_ylabel('Q-Value')
            ax2.set_title('Mean Q-Values')
            ax2.legend()
        else:
            ax2.text(0.5, 0.5, 'No valid Q-value data', ha='center', va='center')
    else:
        ax2.text(0.5, 0.5, 'No valid Q-value data', ha='center', va='center')
    
    # 3. æ’è»Šæ¬¡æ•¸å’Œå®Œæˆç‡
    ax3 = fig.add_subplot(total_rows, 2, 3)
    
    if crashes and len(crashes) > 0:
        # è¨ˆç®—ç´¯ç©æ’è»Šæ¬¡æ•¸
        cum_crashes = np.cumsum(crashes)
        crash_rate = np.array(crashes)
        
        # è¨ˆç®—ç§»å‹•å¹³å‡æ’è»Šç‡ï¼ˆæ¯100å›åˆï¼‰
        window = min(50, len(crash_rate))
        if window > 0:
            crash_moving_avg = [np.mean(crash_rate[max(0, i-window):i+1]) for i in range(len(crash_rate))]
            ax3.plot(crash_moving_avg, 'r-', label='Crash Rate (Moving Avg)')
    
        ax3.set_xlabel('Episode')
        ax3.set_ylabel('Crash Rate')
        ax3.set_title('Crash Statistics')
        ax3.legend()
        
        # ç¬¬äºŒå€‹yè»¸é¡¯ç¤ºç´¯ç©æ’è»Šæ•¸
        ax3b = ax3.twinx()
        ax3b.plot(cum_crashes, 'b--', label='Cumulative Crashes')
        ax3b.set_ylabel('Total Crashes')
        ax3b.legend(loc='upper right')
    else:
        ax3.text(0.5, 0.5, 'No crash data available', ha='center', va='center')
    
    # 4. å®Œæˆç‡
    ax4 = fig.add_subplot(total_rows, 2, 4)
    
    if completions and len(completions) > 0:
        # è¨ˆç®—ç´¯ç©å®Œæˆæ•¸
        cum_completions = np.cumsum(completions)
        completion_rate = np.array(completions)
        
        # è¨ˆç®—ç§»å‹•å¹³å‡å®Œæˆç‡
        window = min(50, len(completion_rate))
        if window > 0:
            completion_moving_avg = [np.mean(completion_rate[max(0, i-window):i+1]) for i in range(len(completion_rate))]
            ax4.plot(completion_moving_avg, 'g-', label='Completion Rate (Moving Avg)')
        
        ax4.set_xlabel('Episode')
        ax4.set_ylabel('Completion Rate')
        ax4.set_title('Lap Completion Statistics')
        ax4.legend()
        
        # ç¬¬äºŒå€‹yè»¸é¡¯ç¤ºç´¯ç©å®Œæˆæ•¸
        ax4b = ax4.twinx()
        ax4b.plot(cum_completions, 'b--', label='Cumulative Completions')
        ax4b.set_ylabel('Total Completions')
        ax4b.legend(loc='upper right')
    else:
        ax4.text(0.5, 0.5, 'No completion data available', ha='center', va='center')
    
    # 5. å¹³å‡é€Ÿåº¦
    ax5 = fig.add_subplot(total_rows, 2, 5)
    if valid_speeds:
        speed_kmh = speeds
        ax5.plot(speed_kmh, 'b-', alpha=0.5, label='Mean Speed (km/h)')
        
        # è¨ˆç®—é€Ÿåº¦çš„ç§»å‹•å¹³å‡
        window = min(50, len(speed_kmh))
        if window > 0:
            speed_moving_avg = [np.mean(speed_kmh[max(0, i-window):i+1]) for i in range(len(speed_kmh))]
            ax5.plot(speed_moving_avg, 'r-', label='Moving Avg')
        
        ax5.set_xlabel('Episode')
        ax5.set_ylabel('Speed (km/h)')
        ax5.set_title('Mean Speed per Episode')
        ax5.legend()
    else:
        print("è­¦å‘Š: æ²’æœ‰æœ‰æ•ˆçš„é€Ÿåº¦æ•¸æ“šç”¨æ–¼ç¹ªåœ–")
        ax5.text(0.5, 0.5, 'No valid speed data', ha='center', va='center')
    
    # 6. é€Ÿåº¦åˆ†å¸ƒ
    ax6 = fig.add_subplot(total_rows, 2, 6)
    speed_bins = ['0-30', '30-60', '60-90', '90+']
    
    if isinstance(speed_dist, np.ndarray) and len(speed_dist) == 4 and not np.isnan(speed_dist).any():
        ax6.bar(speed_bins, speed_dist, color='skyblue')
        ax6.set_xlabel('Speed Range (km/h)')
        ax6.set_ylabel('Percentage')
        ax6.set_title('Speed Distribution')
        
        # é¡¯ç¤ºç™¾åˆ†æ¯”
        for i, v in enumerate(speed_dist):
            ax6.text(i, v + 0.01, f'{v:.1%}', ha='center')
    else:
        ax6.text(0.5, 0.5, 'No valid speed distribution data', ha='center', va='center')
    
    # 7. è³½é“å®Œæˆç™¾åˆ†æ¯”
    ax7 = fig.add_subplot(total_rows, 2, 7)
    
    if completion_percentages and len(completion_percentages) > 0:
        ax7.plot(completion_percentages, 'g-', alpha=0.6, label='Completion (%)')
        
        # è¨ˆç®—å®Œæˆç™¾åˆ†æ¯”çš„ç§»å‹•å¹³å‡
        window = min(50, len(completion_percentages))
        if window > 0:
            completion_moving_avg = [np.mean(completion_percentages[max(0, i-window):i+1]) for i in range(len(completion_percentages))]
            ax7.plot(completion_moving_avg, 'r-', label='Moving Avg')
    else:
        ax7.text(0.5, 0.5, 'No completion percentage data available', ha='center', va='center')
    
    ax7.set_xlabel('Episode')
    ax7.set_ylabel('Completion (%)')
    ax7.set_title('Track Completion Percentage')
    ax7.set_ylim([0, 105])  # è¨­ç½®yè»¸é™åˆ¶ç‚º0-105%
    ax7.legend()
    plt.tight_layout()
    
    # æ ¹æ“šæ˜¯å¦ç‚ºæœ€çµ‚çµ±è¨ˆæ±ºå®šä¿å­˜æ–‡ä»¶å
    if final:
        plt.savefig(f"{save_dir}/final_training_stats.png")
    else:
        plt.savefig(f"{save_dir}/training_stats_ep{episode}.png")
    
    plt.close()  # é—œé–‰åœ–ç‰‡ï¼Œé¿å…memoryæ³„æ¼



def main():
    global start_time
    start_time = time.time()
    
    # æ³¨å†Šè¨Šè™Ÿè™•ç†å™¨ï¼Œæ•ç² CTRL+C å’Œçµ‚æ­¢è¨Šè™Ÿ
    signal.signal(signal.SIGINT, signal_handler)  # CTRL+C
    if hasattr(signal, 'SIGTERM'):
        signal.signal(signal.SIGTERM, signal_handler)  # çµ‚æ­¢è¨Šè™Ÿ
        
    # CUDAè¨­ç½® - é¿å…TF32å°è‡´çš„inf/NaNæº¢å‡º
    if torch.cuda.is_available():
        print("è¨­ç½®CUDAé¸é …ä»¥é¿å…æ•¸å€¼æº¢å‡º")
        torch.backends.cudnn.allow_tf32 = False
        torch.backends.cuda.matmul.allow_tf32 = False
    
    parser = argparse.ArgumentParser(description='è‡ªå‹•é§•é§›è¨“ç·´')
    parser.add_argument('--episodes', type=int, default=4000, help='è¨“ç·´å›åˆæ•¸')
    parser.add_argument('--max_steps', type=int, default=5000, help='æ¯å€‹å›åˆçš„æœ€å¤§æ­¥æ•¸ (é…åˆtimeout)') # 150s timeout / 0.04s/step = 3750 steps. 5000 allows for longer if timeout is increased.
    parser.add_argument('--batch_size', type=int, default=256, help='è¨“ç·´æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--lr', type=float, default=3e-5, help='å­¸ç¿’ç‡ (åŸç‚º1e-4, é™ä½ä»¥å¢åŠ ç©©å®šæ€§)')
    parser.add_argument('--gamma', type=float, default=0.99, help='æŠ˜æ‰£å› å­') 
    parser.add_argument('--epsilon', type=float, default=1.0, help='åˆå§‹æ¢ç´¢ç‡')
    parser.add_argument('--epsilon_decay', type=float, default=0.9975, help='æŒ‡æ•¸è¡°æ¸›æ¨¡å¼ä¸‹çš„è¡°æ¸›ç‡ (åŸç‚º0.997)') # è¼•å¾®èª¿æ•´
    parser.add_argument('--epsilon_min', type=float, default=0.05, help='æœ€å°æ¢ç´¢ç‡') # ä¿æŒæˆ–å¯ç•¥å¢è‡³0.05
    parser.add_argument('--epsilon_decay_mode', type=str, default='exponential', 
                        choices=['exponential', 'linear'], help='æ¢ç´¢ç‡è¡°æ¸›æ¨¡å¼: exponentialæˆ–linear')
    parser.add_argument('--decay_steps', type=int, default=1000000, help='ç·šæ€§è¡°æ¸›æ¨¡å¼ä¸‹çš„è¡°æ¸›æ­¥æ•¸ (å¦‚æœä½¿ç”¨linear mode)')
    parser.add_argument('--buffer_size', type=int, default=250000, help='ç¶“é©—å›æ”¾ç·©è¡å€å¤§å° (åŸ150k-200k, å¯å†ç•¥å¢)')
    parser.add_argument('--update_target_freq', type=int, default=7500, help='ç›®æ¨™ç¶²è·¯æ›´æ–°é »ç‡(æ­¥æ•¸) (åŸç‚º5000, æ¸›æ…¢æ›´æ–°)')
    parser.add_argument('--forward_pretraining', action='store_true', default=True, help='æ˜¯å¦é€²è¡Œå‰é€²å‹•ä½œé è¨“ç·´') # default=True
    parser.add_argument('--pretraining_episodes', type=int, default=30, help='å‰é€²å‹•ä½œé è¨“ç·´çš„å›åˆæ•¸ (æ¸›å°‘, è®“ä¸»è¨“ç·´æ›´å¿«é–‹å§‹)')
    parser.add_argument('--huber_delta', type=float, default=1.0, help='Huberæå¤±çš„deltaå€¼ (SmoothL1Loss beta)')
    parser.add_argument('--grad_clip_norm', type=float, default=5.0, help='æ¢¯åº¦è£å‰ªçš„æœ€å¤§ç¯„æ•¸ (åŸç‚º5.0, å¯ç•¥å¾®æ”¾å¯¬)')
    parser.add_argument('--weight_decay', type=float, default=1e-5, help='AdamWå„ªåŒ–å™¨çš„æ¬Šé‡è¡°æ¸›åƒæ•¸ (ç¨å°)')

    parser.add_argument('--save_freq', type=int, default=100, help='æ¨¡å‹ä¿å­˜é »ç‡(å›åˆ)')
    parser.add_argument('--model_dir', type=str, default='models', help='æ¨¡å‹ä¿å­˜ç›®éŒ„')
    parser.add_argument('--load_model', type=str, default=None, help='è¼‰å…¥å·²æœ‰æ¨¡å‹è·¯å¾‘')
    parser.add_argument('--exe_path', type=str, default='racing.exe', help='è³½è»ŠéŠæˆ²å¯åŸ·è¡Œæ–‡ä»¶è·¯å¾‘')
    parser.add_argument('--warmup_steps', type=int, default=5000, help='Replay-buffer é ç†±æ­¥æ•¸') 
    
    # æ·»åŠ é›»å­éƒµä»¶é€šçŸ¥ç›¸é—œé¸é …
    parser.add_argument('--email_notifications', action='store_true', help='å•Ÿç”¨é›»å­éƒµä»¶é€šçŸ¥')
    parser.add_argument('--email_recipient', type=str, default='C110110157@gmail.com', help='é›»å­éƒµä»¶æ”¶ä»¶äºº')
    parser.add_argument('--email_sender', type=str, default='training.notification@gmail.com', help='ç™¼ä»¶äººé›»å­éƒµä»¶')
    parser.add_argument('--email_password', type=str, default='', help='ç™¼ä»¶äººé›»å­éƒµä»¶å¯†ç¢¼æˆ–æ‡‰ç”¨å¯†ç¢¼')
    parser.add_argument('--notify_lap_completion', action='store_true', help='å®Œæˆä¸€åœˆæ™‚ç™¼é€é€šçŸ¥')
    parser.add_argument('--notify_training_completion', action='store_true', help='è¨“ç·´å®Œæˆæ™‚ç™¼é€é€šçŸ¥')
    parser.add_argument('--notify_errors', action='store_true', help='ç™¼ç”ŸéŒ¯èª¤æ™‚ç™¼é€é€šçŸ¥')
    parser.add_argument('--test_email', action='store_true', help='ç™¼é€æ¸¬è©¦éƒµä»¶ä¸¦é€€å‡ºç¨‹å¼')
    
    args = parser.parse_args()
    
    # åˆå§‹åŒ–é›»å­éƒµä»¶é€šçŸ¥é…ç½®
    global email_config
    email_config = {
        'enabled': args.email_notifications,
        'recipient': args.email_recipient,
        'sender': args.email_sender,
        'password': args.email_password,
        'notify_lap_completion': args.notify_lap_completion,
        'notify_training_completion': args.notify_training_completion,
        'notify_errors': args.notify_errors
    }
    
    # å¦‚æœæŒ‡å®šäº†æ¸¬è©¦éƒµä»¶é¸é …ï¼Œç™¼é€æ¸¬è©¦éƒµä»¶ç„¶å¾Œé€€å‡º
    if args.test_email:
        print("æ­£åœ¨ç™¼é€æ¸¬è©¦éƒµä»¶...")
        if not args.email_password:
            print("éŒ¯èª¤: ä½¿ç”¨--email_passwordåƒæ•¸æä¾›Gmailæ‡‰ç”¨ç¨‹å¼å¯†ç¢¼")
            return
            
        success = send_email_notification(
            subject="è³½è»Šè¨“ç·´é€šçŸ¥: æ¸¬è©¦éƒµä»¶",
            message="é€™æ˜¯ä¸€å°æ¸¬è©¦éƒµä»¶ï¼Œç”¨æ–¼ç¢ºèªé›»å­éƒµä»¶é€šçŸ¥åŠŸèƒ½æ­£å¸¸é‹ä½œã€‚\n\n"
                   f"ç™¼é€æ™‚é–“: {time.strftime('%Y-%m-%d %H:%M:%S')}\n"
                   f"ç™¼ä»¶äºº: {args.email_sender}\n"
                   f"æ”¶ä»¶äºº: {args.email_recipient}"
        )
        
        if success:
            print("æ¸¬è©¦éƒµä»¶ç™¼é€æˆåŠŸ! è«‹æª¢æŸ¥æ‚¨çš„æ”¶ä»¶ç®±ã€‚")
        else:
            print("æ¸¬è©¦éƒµä»¶ç™¼é€å¤±æ•—ã€‚è«‹æª¢æŸ¥ä¸Šæ–¹éŒ¯èª¤è¨Šæ¯ã€‚")
            
        # æ¸¬è©¦éƒµä»¶è¨˜éŒ„åˆ°logs.csv
        log_to_csv(
            time=datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            runtime=time.time() - start_time,
            reason="æ¸¬è©¦éƒµä»¶ç™¼é€å®Œæˆ"
        )
        return
    
    # ç’°å¢ƒåˆå§‹åŒ–
    env = RacingEnvironment(exe_path=args.exe_path)
    state_size = 9
    action_size = env.action_size
    
    
    # æ ¹æ“šé¸æ“‡åˆå§‹åŒ–å°æ‡‰çš„æ™ºèƒ½é«”
    agent = DualOutputDQNAgent(
            state_size=state_size,
            lr=args.lr,
            gamma=args.gamma,
            epsilon=args.epsilon, # ä½¿ç”¨ args.epsilon (é è¨­ç‚º 1.0)
            epsilon_decay=args.epsilon_decay,
            epsilon_min=args.epsilon_min,
            buffer_size=args.buffer_size,
            batch_size=args.batch_size,
            epsilon_decay_mode=args.epsilon_decay_mode,
            decay_steps=args.decay_steps,
            huber_delta=args.huber_delta,
            grad_clip_norm=args.grad_clip_norm,
            weight_decay=args.weight_decay,
            update_target_freq=args.update_target_freq
        )
    
    # å¦‚æœæŒ‡å®šäº†æ¨¡å‹è·¯å¾‘ï¼Œå‰‡è¼‰å…¥æ¨¡å‹
    if args.load_model:
        agent.load(args.load_model)
    
    try:
        # é–‹å§‹è¨“ç·´
            rewards, avg_rewards = train_agent(
                agent=agent,
                env=env,
                num_episodes=args.episodes,
                max_steps_per_episode=args.max_steps,
                save_freq=args.save_freq,
                model_dir=args.model_dir,
                warmup_steps=args.warmup_steps,
                forward_pretraining=args.forward_pretraining,
                pretraining_episodes=args.pretraining_episodes,
                turn_training_episodes=0  # è¨­ç½®ç‚º0ï¼Œä¸ä½¿ç”¨è½‰å‘è¨“ç·´
            )
            
            # ç¹ªè£½è¨“ç·´çå‹µåœ–
            plot_rewards(rewards, avg_rewards)
            
            # è¨˜éŒ„è¨“ç·´å®Œæˆ
            log_to_csv(
                time=datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                runtime=time.time() - start_time,
                reason=f"è¨“ç·´é †åˆ©å®Œæˆ (å…±{args.episodes}å›åˆ)"
            )
        
    except Exception as e:
        error_msg = str(e)
        traceback_str = traceback.format_exc()
        print(f"åŸ·è¡Œéç¨‹ä¸­å‡ºéŒ¯: {error_msg}")
        print(traceback_str)
        
        # è¨˜éŒ„éŒ¯èª¤åˆ°logs.csv
        log_to_csv(
            time=datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            runtime=time.time() - start_time,
            reason=f"è¨“ç·´éŒ¯èª¤ä¸­æ–·: {error_msg}"
        )
        
        # ç™¼é€éŒ¯èª¤é€šçŸ¥
        if 'email_config' in globals() and email_config['enabled'] and email_config['notify_errors']:
            subject = "è³½è»Šè¨“ç·´é€šçŸ¥: è¨“ç·´å‡ºç¾éŒ¯èª¤"
            message = (f"è¨“ç·´ç¨‹å¼å‡ºç¾éŒ¯èª¤ï¼Œå·²è‡ªå‹•åœæ­¢!\n\n"
                      f"éŒ¯èª¤æ™‚é–“: {time.strftime('%Y-%m-%d %H:%M:%S')}\n"
                      f"éŒ¯èª¤è¨Šæ¯: {error_msg}\n\n"
                      f"è©³ç´°å †æ£§è·Ÿè¸ª:\n{traceback_str}")
            
            # å˜—è©¦ç²å–ç•¶å‰åœ–è¡¨ä½œç‚ºé™„ä»¶
            attachments = []
            try:
                if 'model_dir' in locals() and os.path.exists(args.model_dir):
                    plot_files = [f for f in os.listdir(args.model_dir) if f.endswith('.png')]
                    if plot_files:
                        latest_plot = sorted(plot_files)[-1]  # ç²å–æœ€æ–°çš„åœ–è¡¨
                        plot_path = os.path.join(args.model_dir, latest_plot)
                        attachments.append(("error_training_state.png", plot_path))
            except Exception:
                # å¦‚æœç²å–é™„ä»¶å‡ºéŒ¯ï¼Œå¿½ç•¥ä¸¦ç¹¼çºŒç™¼é€éƒµä»¶
                pass
                
            send_email_notification(
                subject=subject,
                message=message,
                attachments=attachments
            )
    finally:
        # é—œé–‰ç’°å¢ƒ
        env.close()

if __name__ == "__main__":
    main()
